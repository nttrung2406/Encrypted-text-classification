{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast, DistilBertForSequenceClassification,\n",
    "    RobertaTokenizerFast, RobertaForSequenceClassification,\n",
    "    XLNetTokenizer, XLNetForSequenceClassification,\n",
    "    ElectraTokenizer, ElectraForSequenceClassification,\n",
    "    DebertaTokenizer, DebertaForSequenceClassification\n",
    ")\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.stats import mode\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import os\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.getcwd()\n",
    "train_path = os.path.join(root, 'training.csv')\n",
    "val_path = os.path.join(root, 'validation.csv')\n",
    "test_path = os.path.join(root, 'test.csv')\n",
    "unlabeled_path = os.path.join(root, 'chunked_unlabeled_texts.csv')\n",
    "train_df = pd.read_csv(train_path)\n",
    "val_df = pd.read_csv(val_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "unlabeled_df = pd.read_csv(unlabeled_path)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_weight = os.path.join(root, 'weight', 'DistilBERT_best_model_state.bin')\n",
    "roberta_weight = os.path.join(root, 'weight', 'RoBERTa_best_model_state.bin')\n",
    "xlnet_weight = os.path.join(root, 'weight', 'XLNet_best_model_state.bin')\n",
    "electra_weight = os.path.join(root, 'weight', 'ELECTRA_best_model_state.bin')\n",
    "deberta_weight = os.path.join(root, 'weight', 'DeBERTa_best_model_state.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_weight_finetune = os.path.join(root, 'finetune_unlabel_weight', 'DistilBERT_best_model_finetune.bin')\n",
    "roberta_weight_finetune = os.path.join(root, 'finetune_unlabel_weight', 'RoBERTa_best_model_finetune.bin')\n",
    "xlnet_weight_finetune = os.path.join(root, 'finetune_unlabel_weight', 'XLNet_best_model_finetune.bin')\n",
    "electra_weight_finetune = os.path.join(root, 'finetune_unlabel_weight', 'ELECTRA_best_model_finetune.bin')\n",
    "deberta_weight_finetune = os.path.join(root, 'finetune_unlabel_weight', 'DeBERTa_best_model_finetune.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "models_tokenizers = {\n",
    "    \"DistilBERT\": (DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased'),\n",
    "                   DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3),\n",
    "                   distilbert_weight_finetune),\n",
    "    \"RoBERTa\": (RobertaTokenizerFast.from_pretrained('roberta-base'),\n",
    "                RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3),\n",
    "                roberta_weight_finetune),\n",
    "    \"XLNet\": (XLNetTokenizer.from_pretrained('xlnet-base-cased'),\n",
    "              XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=3),\n",
    "              xlnet_weight_finetune),\n",
    "    \"ELECTRA\": (ElectraTokenizer.from_pretrained('google/electra-base-discriminator'),\n",
    "                ElectraForSequenceClassification.from_pretrained('google/electra-base-discriminator', num_labels=3),\n",
    "                electra_weight_finetune),\n",
    "    \"DeBERTa\": (DebertaTokenizer.from_pretrained('microsoft/deberta-base'),\n",
    "                DebertaForSequenceClassification.from_pretrained('microsoft/deberta-base', num_labels=3),\n",
    "                deberta_weight_finetune)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df['texts']\n",
    "train_labels = train_df['labels']\n",
    "val_texts = val_df['texts']\n",
    "val_labels = val_df['labels']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, device, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        labels = d[\"labels\"].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "        loss = F.cross_entropy(outputs.logits, labels)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            labels = d[\"labels\"].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            loss = F.cross_entropy(outputs.logits, labels)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            losses.append(loss.item())\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "def train_and_predict(model_name, tokenizer, model, train_texts, train_labels, val_texts, val_labels, test_texts):\n",
    "    print(f\"Training and predicting with {model_name}...\")\n",
    "    \n",
    "    train_dataset = CustomDataset(train_texts, train_labels, tokenizer, max_len=32)\n",
    "    val_dataset = CustomDataset(val_texts, val_labels, tokenizer, max_len=32)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = model.to(device) \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    for epoch in range(50):  \n",
    "        train_acc, train_loss = train_epoch(model, train_loader, optimizer, device, len(train_dataset))\n",
    "        val_acc, val_loss = eval_model(model, val_loader, device, len(val_dataset))\n",
    "        print(f'Epoch {epoch + 1} - {model_name}: Train Acc {train_acc}, Val Acc {val_acc}')\n",
    "        \n",
    "        if val_acc > best_accuracy:\n",
    "            torch.save(model.state_dict(), f'{model_name}_best_model_state.bin')\n",
    "            best_accuracy = val_acc\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"{model_name}_best_model_state.bin\"))\n",
    "    model.eval()\n",
    "    \n",
    "    test_dataset = CustomDataset(test_texts, labels=[0] * len(test_texts), tokenizer=tokenizer, max_len=32)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    test_preds = []\n",
    "    with torch.no_grad():\n",
    "        for d in test_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    test_df[f\"{model_name}_predictions\"] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = test_df[\"texts\"].tolist()\n",
    "for model_name, (tokenizer, model) in models_tokenizers.items():\n",
    "    train_and_predict(model_name, tokenizer, model, train_texts, train_labels, val_texts, val_labels, test_texts)\n",
    "\n",
    "test_df.to_csv(\"all_model_predictions.csv\", index=False)\n",
    "print(\"All model predictions have been saved to 'all_model_predictions.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference finetuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4114 1298 322 76 395 4024 20 2598 3977 20 20 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4114 1298 1297 674 1838 4116 61 1289 84 1445 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96 1707 1295 3457 468 2348 804 1846 4134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4132 146 1839 3413 3977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3352 1001 32 1999 3 74 220 20 1309 390 36 4129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>4114 1298 1846 4134 395 395 395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>2583 3352 1001 20 51 1846 250 2718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>4114 1298 4114 1298 3907 1101 3403 4119 4116 17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>337 1838 4116 36 1935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1631 20 1144 395 395 395 3421 20 3421 20 1266 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 texts\n",
       "0    4114 1298 322 76 395 4024 20 2598 3977 20 20 6...\n",
       "1    4114 1298 1297 674 1838 4116 61 1289 84 1445 4...\n",
       "2            96 1707 1295 3457 468 2348 804 1846 4134 \n",
       "3                             4132 146 1839 3413 3977 \n",
       "4      3352 1001 32 1999 3 74 220 20 1309 390 36 4129 \n",
       "..                                                 ...\n",
       "295                   4114 1298 1846 4134 395 395 395 \n",
       "296                2583 3352 1001 20 51 1846 250 2718 \n",
       "297   4114 1298 4114 1298 3907 1101 3403 4119 4116 17 \n",
       "298                             337 1838 4116 36 1935 \n",
       "299  1631 20 1144 395 395 395 3421 20 3421 20 1266 ...\n",
       "\n",
       "[300 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_1528\\700515767.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(weight_path, map_location=device))\n",
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_1528\\700515767.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(weight_path, map_location=device))\n",
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_1528\\700515767.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(weight_path, map_location=device))\n",
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_1528\\700515767.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(weight_path, map_location=device))\n",
      "C:\\Users\\flori\\AppData\\Local\\Temp\\ipykernel_1528\\700515767.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(weight_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_df['texts'] = test_df['texts'].fillna(\"<EMPTY>\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def get_model_predictions(model, tokenizer, texts, model_name):\n",
    "    predictions = []\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    if isinstance(texts, pd.Series):\n",
    "        texts = texts.astype(str).tolist()  \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            if not isinstance(text, str):\n",
    "                text = str(text)  \n",
    "            \n",
    "            encoding = tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=128,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.append(preds.item())\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "for model_name, (tokenizer, model, weight_path) in models_tokenizers.items():\n",
    "    model.load_state_dict(torch.load(weight_path, map_location=device))\n",
    "    column_name = f\"{model_name}_predictions\"\n",
    "    test_df[column_name] = get_model_predictions(model, tokenizer, test_df['texts'], model_name)\n",
    "\n",
    "test_df['labels'] = test_df[\n",
    "    [f\"{model_name}_predictions\" for model_name in models_tokenizers.keys()]\n",
    "].mode(axis=1)[0]\n",
    "test_df['labels'] = test_df['labels'].astype(int)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
